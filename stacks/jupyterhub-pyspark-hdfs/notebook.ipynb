{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import dayofweek, to_date, to_timestamp, year, hour, minute, month, when, dayofmonth, dayofweek\n",
    "from pyspark.sql.functions import concat_ws, lpad, lit\n",
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions, types\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this please if using non-default product namespace for demo deploynig \n",
    "NAMESPACE: str = \"default\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .master(f'k8s://https://{os.environ[\"KUBERNETES_SERVICE_HOST\"]}:{os.environ[\"KUBERNETES_SERVICE_PORT\"]}')\n",
    "        .config(\"spark.kubernetes.container.image\", \"docker.stackable.tech/demos/spark-k8s-with-scikit-learn:3.5.0-stackable24.3.0\")\n",
    "        .config(\"spark.driver.port\", \"2222\")\n",
    "        .config(\"spark.driver.blockManager.port\", \"7777\")\n",
    "        .config(\"spark.driver.host\", f\"driver-service.{NAMESPACE}.svc.cluster.local\")\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "        .config(\"spark.kubernetes.namespace\", NAMESPACE)\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "        .config(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "        .config(\"spark.executor.instances\", \"4\")\n",
    "        .config(\"spark.kubernetes.container.image.pullPolicy\", \"IfNotPresent\")\n",
    "        .appName(\"taxi-data-anomaly-detection\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.read.parquet(\"hdfs://hdfs/ny-taxi-data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = input_df.select(\n",
    "        to_date(input_df.pickup_datetime).alias(\"day_date\")\n",
    "        , year(input_df.pickup_datetime).alias('year')\n",
    "        , month(input_df.pickup_datetime).alias('month')\n",
    "        , dayofmonth(input_df.pickup_datetime).alias(\"dayofmonth\")\n",
    "        , dayofweek(input_df.pickup_datetime).alias(\"dayofweek\")\n",
    "        , hour(input_df.pickup_datetime).alias(\"hour\")\n",
    "        , minute(input_df.pickup_datetime).alias(\"minute\")\n",
    "        , input_df.driver_pay\n",
    "    )\n",
    "    \n",
    "df = df.withColumn(\"minute_group\", when(df.minute < 30, '00').otherwise('30'))\n",
    "df = df.withColumn(\"time_group\",concat_ws(\":\", lpad(df.hour, 2, '0'), df.minute_group, lit('00')))\n",
    "df = df.withColumn(\"ts\",concat_ws(\" \", df.day_date, df.time_group))\n",
    "    \n",
    "dfs = df.select(\n",
    "        to_timestamp(df.ts, \"yyyy-MM-dd HH:mm:ss\").alias(\"date_group\")\n",
    "        , df.minute_group\n",
    "        , df.year\n",
    "        , df.hour\n",
    "        , df.month\n",
    "        , df.dayofmonth\n",
    "        , df.dayofweek\n",
    "        , df.driver_pay\n",
    ").groupby(\"date_group\", \"minute_group\", \"hour\", \"year\", \"month\", \"dayofmonth\", \"dayofweek\").agg(functions.count('driver_pay').alias('no_rides'), functions.round(functions.sum('driver_pay'), 2).alias('total_bill'), functions.round(functions.avg('driver_pay'), 2).alias('avg_bill')).orderBy(\"date_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "windowSpec  = Window.partitionBy(\"hour\").orderBy(\"date_group\")\n",
    "    \n",
    "dfs = dfs.withColumn(\"lag\",lag(\"no_rides\",2).over(windowSpec))\n",
    "dfs = dfs.filter(\"lag IS NOT NULL\")\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "classifier = IsolationForest(contamination=0.005, n_estimators=200, max_samples=0.7, random_state=42, n_jobs=-1)\n",
    "    \n",
    "df_model = dfs.select(dfs.minute_group, dfs.hour, dfs.year, dfs.month, dfs.dayofmonth, dfs.dayofweek, dfs.no_rides, dfs.total_bill, dfs.avg_bill, dfs.lag)\n",
    "    \n",
    "x_train = scaler.fit_transform(df_model.collect())\n",
    "clf = classifier.fit(x_train)\n",
    "    \n",
    "SCL = spark.sparkContext.broadcast(scaler)\n",
    "CLF = spark.sparkContext.broadcast(clf)\n",
    "    \n",
    "def predict_using_broadcasts(minute_group, hour, year, month, dayofmonth, dayofweek, no_rides, total_bill, avg_bill, lag):\n",
    "    prediction = 0\n",
    "    x_test = [[minute_group, hour, year, month, dayofmonth, dayofweek, no_rides, total_bill, avg_bill, lag]]\n",
    "    try:\n",
    "        x_test = SCL.value.transform(x_test)\n",
    "        prediction = CLF.value.predict(x_test)[0]\n",
    "    except ValueError:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print('Cannot predict:', x_test)\n",
    "    return int(prediction)\n",
    "    \n",
    "udf_predict_using_broadcasts = functions.udf(predict_using_broadcasts, types.IntegerType())\n",
    "\n",
    "df_pred = dfs.withColumn(\n",
    "    'prediction',\n",
    "    udf_predict_using_broadcasts('minute_group', 'hour', 'year', 'month', 'dayofmonth', 'dayofweek', 'no_rides', 'total_bill', 'avg_bill', 'lag')\n",
    ")\n",
    "\n",
    "# map to table columns\n",
    "df_out = df_pred.select(\n",
    "    df_pred.date_group.alias(\"pickup_ts\")\n",
    "    , df_pred.minute_group.alias(\"pickup_minute_group\")\n",
    "    , df_pred.hour.alias(\"pickup_hour\")\n",
    "    , df_pred.year.alias(\"pickup_year\")\n",
    "    , df_pred.month.alias(\"pickup_month\")\n",
    "    , df_pred.dayofmonth.alias(\"pickup_dayofmonth\")\n",
    "    , df_pred.dayofweek.alias(\"pickup_dayofweek\")\n",
    "    , df_pred.no_rides.alias(\"norides\")\n",
    "    , df_pred.total_bill.alias(\"total_bill\")\n",
    "    , df_pred.avg_bill.alias(\"avg_bill\")\n",
    "    , df_pred.lag.alias(\"norides_lag\")\n",
    "    , df_pred.prediction.alias(\"pred\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df_out.toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display this data by plotting some metrics against time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(df_line):\n",
    "    df_outliers = df_line[df_line[\"pred\"]==-1]\n",
    "\n",
    "    plt.figure(figsize=(24, 4))\n",
    "    ax1 = plt.subplot()\n",
    "    plt.plot(df_line.pickup_ts, df_line.norides, linewidth = 1.4)\n",
    "    plt.plot(df_outliers.pickup_ts, df_outliers.norides, 'o', markersize=12, color='black')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('number of rides',color='b')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_line.pickup_ts, df_line.avg_bill, 'g-', linewidth = 0.9)\n",
    "    ax2.spines['right'].set_position(('outward', 60))\n",
    "    plt.ylabel('average bill', color='g')\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.plot(df_line.pickup_ts, df_line.total_bill, 'r-', linewidth = 1.2)\n",
    "    ax3.spines['right'].set_position(('outward', 120))\n",
    "    plt.ylabel('total bill', color='r')\n",
    "\n",
    "    plt.title('Taxi metrics per 30-minute slot')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will plot three metrics (number of rides, total and average bill) for all time slots. The outliers are marked by the black circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfp.copy()\n",
    "df.sort_values(by='pickup_ts', inplace=True)\n",
    "plot_metrics(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in on those outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(df[-240:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above display the outliers mapped onto a particular metric (number of rides), although the anomaly itself is based on all available features. We should really define outliers in terms of the whole feature space, but this is then difficult to visualize due to the *number* of features, each of which is a separate dimension (and this feature set is fairly simple set - some data describing the transaction, different elements of the timestamp, plus one windowed value to add a time-series aspect).\n",
    "\n",
    "We can visualize the outliers by using a Principle Component Analysis to first reduce these features to a dimensionality that that be plotted. Here is a 3-D representation, that shows the outliers in red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = dfp.copy()\n",
    "\n",
    "outliers = df_pca.loc[df_pca['pred']==-1]\n",
    "outlier_index=list(outliers.index)\n",
    "inliers = df_pca.loc[df_pca['pred']==1]\n",
    "inlier_index=list(inliers.index)\n",
    "print(df_pca['pred'].value_counts())\n",
    "\n",
    "pca = PCA(n_components=3)  # Reduce to k=3 dimensions\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# remove the date column so we can use PCA\n",
    "dfplt = df_pca.drop(['pickup_ts'], axis=1)\n",
    "\n",
    "X = scaler.fit_transform(dfplt)\n",
    "X_reduce = pca.fit_transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "_ = ax = fig.add_subplot(111, projection='3d')\n",
    "_ = ax.set_xlabel(\"IsolationForest - 3D\")\n",
    "\n",
    "# Plot the compressed data points\n",
    "_ = ax.scatter(X_reduce[inlier_index, 0], X_reduce[inlier_index, 1], X_reduce[inlier_index, 2], s=12, lw=1, c=\"green\", label=\"normal\")\n",
    "\n",
    "# Plot ground truth outliers\n",
    "_ = ax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1], X_reduce[outlier_index,2], s=24, lw=1, c=\"red\", label=\"outliers\")\n",
    "\n",
    "_ = ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can do the same with two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2) # Reduce to k=2 dimensions\n",
    "res = pd.DataFrame(pca.fit_transform(X))\n",
    "\n",
    "_ = plt.title(\"IsolationForest - 2D\")\n",
    "_ = plt.scatter(res.iloc[inlier_index,0], res.iloc[inlier_index,1], c='green', s=16, label=\"normal\")\n",
    "_ = plt.scatter(res.iloc[outlier_index,0], res.iloc[outlier_index,1], c='red', s=20, edgecolor=\"red\", label=\"outliers\")\n",
    "_ = plt.legend()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0cce569e30cfc77982e12558b223aaebd4eb54319449f822db4339caea4dd42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
